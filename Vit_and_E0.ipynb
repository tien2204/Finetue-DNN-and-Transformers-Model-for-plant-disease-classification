{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ps0R78f9Bmfi6ME7Rulqw3bkZXxYXdTr",
      "authorship_tag": "ABX9TyO4ZA1ktFM8Vy39hL5oksme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tien2204/Finetue-DNN-and-Transformers-Model-for-plant-disease-classification/blob/main/Vit_and_E0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvKXczEtJb0H",
        "outputId": "994d5735-fa0f-4c2a-8f0a-13c99a7edcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available. Using 1 GPU(s).\n",
            "  GPU 0: Tesla T4\n",
            "============================================================\n",
            " PLANT DISEASE CLASSIFICATION TRAINING SCRIPT (PREPROCESSED DATA) \n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "Processing Model: efficientnet_b0\n",
            "==================================================\n",
            "\n",
            "Config -> Accumulation: 1, Model Input Size: (224, 224)\n",
            "\n",
            "--- Loading Preprocessed DataLoaders ---\n",
            "\n",
            "Loading preprocessed data from: /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed\n",
            "Loaded 44018 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/train\n",
            "Loaded 5502 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/val\n",
            "Loaded 5502 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/test\n",
            "Classes loaded (39): ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'background']\n",
            "Created DataLoaders.\n",
            "Number of classes for model: 39\n",
            "\n",
            "Setting up efficientnet_b0...\n",
            "ModelSetup: Loading pre-trained core model: efficientnet_b0\n",
            "Weights enum 'EFFICIENTNETB0_Weights' not found. Trying legacy pretrained=True (may show warning).\n",
            "ModelSetup: Determined in_features for final layer: 1000\n",
            "\n",
            "--- Starting Training (10 epochs) ---\n",
            "Effective Batch Size: 32\n",
            "Using Mixed Precision (AMP): True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/10 Train: 100%|██████████| 1376/1376 [42:17<00:00,  1.84s/batch, acc=0.944, loss=0.1419]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10 Summary -> Train Loss: 0.2657, Train Acc: 0.9229 | Val Loss: 0.1754, Val Acc: 0.9573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 Train: 100%|██████████| 1376/1376 [03:37<00:00,  6.33batch/s, acc=0.889, loss=0.3835]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/10 Summary -> Train Loss: 0.1456, Train Acc: 0.9574 | Val Loss: 0.0846, Val Acc: 0.9775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 Train: 100%|██████████| 1376/1376 [03:40<00:00,  6.25batch/s, acc=1.000, loss=0.0062]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/10 Summary -> Train Loss: 0.1054, Train Acc: 0.9702 | Val Loss: 0.0660, Val Acc: 0.9807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 Train: 100%|██████████| 1376/1376 [03:40<00:00,  6.24batch/s, acc=1.000, loss=0.0655]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/10 Summary -> Train Loss: 0.1093, Train Acc: 0.9693 | Val Loss: 0.7189, Val Acc: 0.9589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 Train: 100%|██████████| 1376/1376 [03:39<00:00,  6.28batch/s, acc=0.944, loss=0.1807]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/10 Summary -> Train Loss: 0.0835, Train Acc: 0.9765 | Val Loss: 0.0928, Val Acc: 0.9784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 Train: 100%|██████████| 1376/1376 [03:56<00:00,  5.83batch/s, acc=0.944, loss=0.2174]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/10 Summary -> Train Loss: 0.0964, Train Acc: 0.9745 | Val Loss: 0.1033, Val Acc: 0.9742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 Train: 100%|██████████| 1376/1376 [03:44<00:00,  6.13batch/s, acc=1.000, loss=0.0216]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/10 Summary -> Train Loss: 0.0735, Train Acc: 0.9796 | Val Loss: 0.2685, Val Acc: 0.9824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 Train: 100%|██████████| 1376/1376 [03:40<00:00,  6.24batch/s, acc=0.944, loss=0.0523]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/10 Summary -> Train Loss: 0.0681, Train Acc: 0.9816 | Val Loss: 0.1079, Val Acc: 0.9727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 Train: 100%|██████████| 1376/1376 [03:41<00:00,  6.22batch/s, acc=1.000, loss=0.0020]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/10 Summary -> Train Loss: 0.0709, Train Acc: 0.9809 | Val Loss: 0.0643, Val Acc: 0.9816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 Train: 100%|██████████| 1376/1376 [03:40<00:00,  6.23batch/s, acc=1.000, loss=0.0005]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/10 Summary -> Train Loss: 0.0534, Train Acc: 0.9858 | Val Loss: 0.0533, Val Acc: 0.9884\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Final Testing Phase ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Test Results for efficientnet_b0: Loss = 0.0383, Accuracy = 0.9893\n",
            "--------------------------------------------------\n",
            "Cleaning up resources for efficientnet_b0...\n",
            "\n",
            "==================================================\n",
            "Processing Model: vit_b_16\n",
            "==================================================\n",
            "\n",
            "Config -> Accumulation: 4, Model Input Size: (224, 224)\n",
            "\n",
            "--- Loading Preprocessed DataLoaders ---\n",
            "\n",
            "Loading preprocessed data from: /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed\n",
            "Loaded 44018 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/train\n",
            "Loaded 5502 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/val\n",
            "Loaded 5502 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/test\n",
            "Classes loaded (39): ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'background']\n",
            "Created DataLoaders.\n",
            "Number of classes for model: 39\n",
            "\n",
            "Setting up vit_b_16...\n",
            "ModelSetup: Loading pre-trained core model: vit_b_16\n",
            "Weights enum 'VITB16_Weights' not found. Trying legacy pretrained=True (may show warning).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 133MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelSetup: Determined in_features for final layer: 1000\n",
            "\n",
            "--- Starting Training (10 epochs) ---\n",
            "Effective Batch Size: 128\n",
            "Using Mixed Precision (AMP): True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 Train: 100%|██████████| 1376/1376 [06:48<00:00,  3.37batch/s, acc=0.833, loss=0.4682]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10 Summary -> Train Loss: 2.0486, Train Acc: 0.4363 | Val Loss: 0.9081, Val Acc: 0.7203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 Train: 100%|██████████| 1376/1376 [06:48<00:00,  3.37batch/s, acc=0.778, loss=0.4479]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/10 Summary -> Train Loss: 0.6326, Train Acc: 0.8029 | Val Loss: 0.4580, Val Acc: 0.8490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 Train: 100%|██████████| 1376/1376 [06:45<00:00,  3.39batch/s, acc=1.000, loss=0.0982]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/10 Summary -> Train Loss: 0.3767, Train Acc: 0.8786 | Val Loss: 0.3210, Val Acc: 0.8957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 Train: 100%|██████████| 1376/1376 [06:44<00:00,  3.40batch/s, acc=0.944, loss=0.1332]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/10 Summary -> Train Loss: 0.2845, Train Acc: 0.9092 | Val Loss: 0.2856, Val Acc: 0.9057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 Train: 100%|██████████| 1376/1376 [06:43<00:00,  3.41batch/s, acc=0.833, loss=0.9200]\n",
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/10 Summary -> Train Loss: 0.2297, Train Acc: 0.9240 | Val Loss: 0.2320, Val Acc: 0.9280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 Train:  39%|███▉      | 538/1376 [02:37<04:05,  3.41batch/s, acc=0.812, loss=0.6906]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR] Train/Val failed: Caught FileNotFoundError in DataLoader worker process 2.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n",
            "    sample = self.loader(path)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n",
            "    return pil_loader(path)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 262, in pil_loader\n",
            "    with open(path, \"rb\") as f:\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/train/Tomato___Late_blight/784f6313-1080-4bd1-a556-d4e14e6879d0___GHLB_PS Leaf 53.1 Day 18 .jpg'\n",
            "\n",
            "Skipping testing.\n",
            "\n",
            "========================================\n",
            "===== TRAINING & TESTING COMPLETE ======\n",
            "========================================\n",
            "Final Test Results Summary:\n",
            "- efficientnet_b0: Test Loss=0.0383, Test Accuracy=0.9893\n",
            "========================================\n",
            "SCRIPT FINISHED\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-2-1bf675081f98>\", line 323, in <cell line: 0>\n",
            "    train_and_validate(\n",
            "  File \"<ipython-input-2-1bf675081f98>\", line 214, in train_and_validate\n",
            "    for i, (inputs, labels) in progress_bar:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "    return self._process_data(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "    raise exception\n",
            "FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 2.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n",
            "    sample = self.loader(path)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n",
            "    return pil_loader(path)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 262, in pil_loader\n",
            "    with open(path, \"rb\") as f:\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/train/Tomato___Late_blight/784f6313-1080-4bd1-a556-d4e14e6879d0___GHLB_PS Leaf 53.1 Day 18 .jpg'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# file: train_evaluate.py\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 1: IMPORTS VÀ CÀI ĐẶT BAN ĐẦU\n",
        "# ==============================================================================\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import traceback\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "# !!! Thay đổi import cho AMP !!!\n",
        "import torch.amp # Import cấp cao hơn\n",
        "# --------------------------------\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Cài đặt cơ bản ---\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"CUDA available. Using {torch.cuda.device_count()} GPU(s).\")\n",
        "    # Thêm thông tin về GPU đang dùng (nếu có)\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"CUDA not available. Using CPU.\")\n",
        "use_gpu = torch.cuda.is_available()\n",
        "# Xác định device type cho amp\n",
        "amp_device_type = 'cuda' if use_gpu else 'cpu'\n",
        "\n",
        "\n",
        "# --- Cấu hình Model và Training ---\n",
        "models_to_test = ['efficientnet_b0', 'vit_b_16']\n",
        "input_sizes = {\n",
        "    'efficientnet_b0': (224, 224),\n",
        "    'vit_b_16': (224, 224)\n",
        "}\n",
        "batch_size = 32\n",
        "# --- Gợi ý tối ưu tốc độ: Thử tăng num_workers ---\n",
        "num_workers_loader = 4 # <<< Thử tăng lên 4 hoặc 8\n",
        "# --------------------------------------------------\n",
        "accumulation_settings = {\n",
        "    'efficientnet_b0': 1,\n",
        "    'vit_b_16': 4\n",
        "}\n",
        "epochs_to_train = 10\n",
        "PROCESSED_DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed' # <<< Đảm bảo đường dẫn đúng\n",
        "num_classes = None\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 2: ĐỊNH NGHĨA MODEL (ExtendedModel) - Giữ nguyên\n",
        "# ==============================================================================\n",
        "class ExtendedModel(nn.Module):\n",
        "    def __init__(self, base_model, num_classes, input_size=(224,224)):\n",
        "        super(ExtendedModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        probe_device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "        # Tạm chuyển base model sang device để probe output size\n",
        "        self.base_model.to(probe_device)\n",
        "        dummy = torch.zeros(1, 3, input_size[0], input_size[1]).to(probe_device)\n",
        "        with torch.no_grad():\n",
        "            dummy_out = self.base_model(dummy)\n",
        "        in_features = dummy_out.shape[1]\n",
        "        print(f\"ModelSetup: Determined in_features for final layer: {in_features}\")\n",
        "        self.extension = nn.Linear(in_features, num_classes)\n",
        "        # Có thể chuyển base_model về CPU ở đây nếu muốn, nhưng không bắt buộc\n",
        "        # self.base_model.to('cpu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        base_out = self.base_model(x)\n",
        "        out = self.extension(base_out)\n",
        "        return out\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 3: HÀM HỖ TRỢ (Load model pre-trained) - Sửa lỗi Warning\n",
        "# ==============================================================================\n",
        "def load_pretrained_core_model(name):\n",
        "    \"\"\"Load model pre-trained từ torchvision, ưu tiên dùng weights API.\"\"\"\n",
        "    print(f\"ModelSetup: Loading pre-trained core model: {name}\")\n",
        "    model = None\n",
        "    try:\n",
        "        # Thử tìm Weights enum tương ứng (ví dụ: EfficientNet_B0_Weights)\n",
        "        weights_enum_name = f\"{name.replace('_', '').upper()}_Weights\" # Tạo tên enum chuẩn hơn\n",
        "        weights_enum = getattr(models, weights_enum_name, None)\n",
        "\n",
        "        if weights_enum:\n",
        "            # Lấy weights mặc định (thường là tốt nhất)\n",
        "            weights = weights_enum.DEFAULT\n",
        "            print(f\"Using weights API: {weights}\")\n",
        "            model = getattr(models, name)(weights=weights)\n",
        "        else:\n",
        "            # Nếu không có weights enum (model cũ hoặc tên không khớp), thử pretrained=True (sẽ có warning)\n",
        "            print(f\"Weights enum '{weights_enum_name}' not found. Trying legacy pretrained=True (may show warning).\")\n",
        "            model = getattr(models, name)(pretrained=True)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        # Xử lý nếu getattr(models, name) thất bại\n",
        "        print(f\"[ERROR] Could not load model '{name}' using standard methods: {e}\")\n",
        "        print(\"Trying models.__dict__ fallback...\")\n",
        "        try:\n",
        "            # Fallback rất cũ (ít dùng)\n",
        "             model = models.__dict__[name](pretrained=True)\n",
        "        except Exception as e2:\n",
        "             print(f\"[ERROR] Failed loading model '{name}' completely: {e2}\")\n",
        "             raise # Ném lỗi ra ngoài nếu không thể load\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] An unexpected error occurred loading model '{name}': {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "    if model is None:\n",
        "        raise ValueError(f\"Could not load model for name: {name}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 4: HÀM TẢI DỮ LIỆU (load_preprocessed_data) - Giữ nguyên logic\n",
        "# ==============================================================================\n",
        "def load_preprocessed_data(processed_data_dir, batch_size, num_workers=4, pin_memory=False):\n",
        "    \"\"\"Tải dữ liệu ĐÃ TIỀN XỬ LÝ.\"\"\"\n",
        "    print(f\"\\nLoading preprocessed data from: {processed_data_dir}\")\n",
        "    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    data_transforms_online = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]),\n",
        "        'val_test': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]),\n",
        "    }\n",
        "    image_datasets = {}\n",
        "    for x in ['train', 'val', 'test']:\n",
        "        split_path = os.path.join(processed_data_dir, x)\n",
        "        if not os.path.isdir(split_path): raise FileNotFoundError(f\"Dir not found: {split_path}\")\n",
        "        try:\n",
        "             transform_key = 'train' if x == 'train' else 'val_test'\n",
        "             image_datasets[x] = datasets.ImageFolder(split_path, data_transforms_online[transform_key])\n",
        "             print(f\"Loaded {len(image_datasets[x])} images from {split_path}\")\n",
        "             if len(image_datasets[x]) == 0: print(f\"[Warning] No images loaded for split '{x}'.\")\n",
        "        except Exception as e: print(f\"[ERROR] Load failed for '{x}': {e}\"); raise\n",
        "    dset_classes = image_datasets['train'].classes\n",
        "    print(f\"Classes loaded ({len(dset_classes)}): {dset_classes}\")\n",
        "    dataloaders = {\n",
        "        x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=(x == 'train'),\n",
        "                      num_workers=num_workers, pin_memory=pin_memory)\n",
        "        for x in ['train', 'val', 'test']\n",
        "    }\n",
        "    print(\"Created DataLoaders.\")\n",
        "    return dataloaders['train'], dataloaders['val'], dataloaders['test'], dset_classes\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 5: HÀM ĐÁNH GIÁ MODEL (evaluate_model) - Sửa lỗi Warning AMP\n",
        "# ==============================================================================\n",
        "def evaluate_model(net, dataloader, criterion):\n",
        "    \"\"\"Đánh giá model trên dataloader.\"\"\"\n",
        "    net.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    device = next(net.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False, unit=\"batch\")\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            # !!! Sửa lỗi Warning: Dùng torch.amp.autocast !!!\n",
        "            with torch.amp.autocast(device_type=amp_device_type, enabled=use_gpu):\n",
        "                outputs = net(inputs)\n",
        "                if criterion:\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_batch = (predicted == labels).sum().item()\n",
        "            total_correct += correct_batch\n",
        "            total_samples += labels.size(0)\n",
        "    avg_loss = total_loss / total_samples if criterion and total_samples > 0 else float('nan')\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 6: HÀM HUẤN LUYỆN VÀ VALIDATE (train_and_validate) - Sửa lỗi Warning AMP\n",
        "# ==============================================================================\n",
        "def train_and_validate(net, trainloader, valloader, criterion, epochs, accumulation_steps):\n",
        "    \"\"\"Huấn luyện model, đánh giá trên validation set sau mỗi epoch.\"\"\"\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    # !!! Sửa lỗi Warning: Dùng torch.amp.GradScaler !!!\n",
        "    scaler = torch.amp.GradScaler(device=amp_device_type, enabled=use_gpu)\n",
        "    # --------------------------------------------------\n",
        "    print(f\"\\n--- Starting Training ({epochs} epochs) ---\")\n",
        "    print(f\"Effective Batch Size: {trainloader.batch_size * accumulation_steps}\")\n",
        "    print(f\"Using Mixed Precision (AMP): {use_gpu}\")\n",
        "    device = next(net.parameters()).device\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        total_train = 0\n",
        "        correct_train = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch+1}/{epochs} Train\", unit=\"batch\")\n",
        "        for i, (inputs, labels) in progress_bar:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # !!! Sửa lỗi Warning: Dùng torch.amp.autocast !!!\n",
        "            with torch.amp.autocast(device_type=amp_device_type, enabled=use_gpu):\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                if accumulation_steps > 1: loss = loss / accumulation_steps\n",
        "            # --------------------------------------------------\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(trainloader):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = loss.item() * (accumulation_steps if accumulation_steps > 1 else 1)\n",
        "            running_loss += batch_loss * inputs.size(0)\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                 _, predicted = torch.max(outputs.data, 1)\n",
        "                 correct_train_batch = (predicted == labels).sum().item()\n",
        "                 batch_acc = correct_train_batch / inputs.size(0) if inputs.size(0) > 0 else 0\n",
        "                 correct_train += correct_train_batch\n",
        "                 progress_bar.set_postfix(loss=f\"{batch_loss:.4f}\", acc=f\"{batch_acc:.3f}\")\n",
        "\n",
        "        train_loss = running_loss / total_train if total_train > 0 else float('nan')\n",
        "        train_acc = correct_train / total_train if total_train > 0 else 0.0\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(net, valloader, criterion)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs} Summary -> \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(\"--- Training Finished ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 7: SCRIPT CHÍNH ĐỂ THỰC THI\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\" PLANT DISEASE CLASSIFICATION TRAINING SCRIPT (PREPROCESSED DATA) \".center(60, \"=\"))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.isdir(PROCESSED_DATA_DIR):\n",
        "        print(f\"[FATAL ERROR] Preprocessed data directory not found: {PROCESSED_DATA_DIR}\")\n",
        "        print(\"Please run the preprocessing script ('preprocess_data*.py') first.\")\n",
        "        exit()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name in models_to_test:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Processing Model: {model_name}\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "        acc_steps = accumulation_settings.get(model_name, 1)\n",
        "        model_input_resize = input_sizes[model_name]\n",
        "        print(f\"Config -> Accumulation: {acc_steps}, Model Input Size: {model_input_resize}\")\n",
        "\n",
        "        # --- Tải Dữ liệu ĐÃ TIỀN XỬ LÝ ---\n",
        "        print(\"\\n--- Loading Preprocessed DataLoaders ---\")\n",
        "        try:\n",
        "            train_loader, val_loader, test_loader, dset_classes = load_preprocessed_data(\n",
        "                processed_data_dir=PROCESSED_DATA_DIR,\n",
        "                batch_size=batch_size,\n",
        "                num_workers=num_workers_loader,\n",
        "                pin_memory=use_gpu\n",
        "            )\n",
        "            if num_classes is None: num_classes = len(dset_classes)\n",
        "            elif num_classes != len(dset_classes):\n",
        "                print(f\"[Warning] Class count mismatch. Resetting num_classes to {len(dset_classes)}\")\n",
        "                num_classes = len(dset_classes)\n",
        "            print(f\"Number of classes for model: {num_classes}\")\n",
        "\n",
        "        except Exception as e: print(f\"[ERROR] Load data failed: {e}\"); traceback.print_exc(); continue\n",
        "\n",
        "        # --- Cài đặt Model ---\n",
        "        try:\n",
        "            print(f\"\\nSetting up {model_name}...\")\n",
        "            base_model = load_pretrained_core_model(model_name)\n",
        "            model = ExtendedModel(base_model, num_classes, input_size=model_input_resize)\n",
        "\n",
        "            # --- Gợi ý tối ưu tốc độ: Thử torch.compile ---\n",
        "            try:\n",
        "                # Chỉ dùng nếu PyTorch version >= 2.0\n",
        "                # print(\"Attempting to compile model with torch.compile()...\")\n",
        "                # model = torch.compile(model)\n",
        "                # print(\"Model compiled successfully.\")\n",
        "                pass # Bỏ comment các dòng trên để kích hoạt\n",
        "            except Exception as compile_err:\n",
        "                print(f\"[Warning] torch.compile() failed: {compile_err}. Proceeding without compiling.\")\n",
        "            # --------------------------------------------\n",
        "\n",
        "            if use_gpu:\n",
        "                if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
        "                model = model.cuda()\n",
        "\n",
        "        except Exception as e: print(f\"[ERROR] Model setup failed: {e}\"); traceback.print_exc(); continue\n",
        "\n",
        "        # --- Huấn luyện và Đánh giá ---\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        if use_gpu: criterion = criterion.cuda()\n",
        "\n",
        "        try:\n",
        "            train_and_validate(\n",
        "                net=model, trainloader=train_loader, valloader=val_loader,\n",
        "                criterion=criterion, epochs=epochs_to_train, accumulation_steps=acc_steps\n",
        "            )\n",
        "        except Exception as e: print(f\"[ERROR] Train/Val failed: {e}\"); traceback.print_exc(); print(\"Skipping testing.\"); continue\n",
        "\n",
        "        # --- Test cuối cùng ---\n",
        "        print(\"\\n--- Final Testing Phase ---\")\n",
        "        try:\n",
        "            test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
        "            print(f\"\\n>>> Test Results for {model_name}: Loss = {test_loss:.4f}, Accuracy = {test_acc:.4f}\")\n",
        "            results[model_name] = {'test_loss': test_loss, 'test_accuracy': test_acc}\n",
        "        except Exception as e: print(f\"[ERROR] Test failed: {e}\"); traceback.print_exc(); results[model_name] = {'test_loss': float('nan'), 'test_accuracy': float('nan')}\n",
        "\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        # --- Dọn dẹp bộ nhớ ---\n",
        "        print(f\"Cleaning up resources for {model_name}...\")\n",
        "        del model, base_model, train_loader, val_loader, test_loader, criterion\n",
        "        if use_gpu: torch.cuda.empty_cache()\n",
        "\n",
        "    # --- In tổng kết ---\n",
        "    print(\"\\n========================================\")\n",
        "    print(\" TRAINING & TESTING COMPLETE \".center(40, \"=\"))\n",
        "    print(\"========================================\")\n",
        "    print(\"Final Test Results Summary:\")\n",
        "    if results:\n",
        "        for model_name, metrics in results.items():\n",
        "             print(f\"- {model_name}: Test Loss={metrics['test_loss']:.4f}, Test Accuracy={metrics['test_accuracy']:.4f}\")\n",
        "    else:\n",
        "        print(\"No models were successfully processed.\")\n",
        "    print(\"=\"*40)\n",
        "    print(\"SCRIPT FINISHED\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file: train_evaluate.py\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 1: IMPORTS VÀ CÀI ĐẶT BAN ĐẦU\n",
        "# ==============================================================================\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import traceback\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "# !!! Thay đổi import cho AMP !!!\n",
        "import torch.amp # Import cấp cao hơn\n",
        "# --------------------------------\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Cài đặt cơ bản ---\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"CUDA available. Using {torch.cuda.device_count()} GPU(s).\")\n",
        "    # Thêm thông tin về GPU đang dùng (nếu có)\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"CUDA not available. Using CPU.\")\n",
        "use_gpu = torch.cuda.is_available()\n",
        "# Xác định device type cho amp\n",
        "amp_device_type = 'cuda' if use_gpu else 'cpu'\n",
        "\n",
        "\n",
        "# --- Cấu hình Model và Training ---\n",
        "models_to_test = ['vit_b_16']\n",
        "input_sizes = {\n",
        "    'vit_b_16': (224, 224)\n",
        "}\n",
        "batch_size = 32\n",
        "# --- Gợi ý tối ưu tốc độ: Thử tăng num_workers ---\n",
        "num_workers_loader = 4 # <<< Thử tăng lên 4 hoặc 8\n",
        "# --------------------------------------------------\n",
        "accumulation_settings = {\n",
        "    'vit_b_16': 4\n",
        "}\n",
        "epochs_to_train = 10\n",
        "PROCESSED_DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed' # <<< Đảm bảo đường dẫn đúng\n",
        "num_classes = None\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 2: ĐỊNH NGHĨA MODEL (ExtendedModel) - Giữ nguyên\n",
        "# ==============================================================================\n",
        "class ExtendedModel(nn.Module):\n",
        "    def __init__(self, base_model, num_classes, input_size=(224,224)):\n",
        "        super(ExtendedModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        probe_device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "        # Tạm chuyển base model sang device để probe output size\n",
        "        self.base_model.to(probe_device)\n",
        "        dummy = torch.zeros(1, 3, input_size[0], input_size[1]).to(probe_device)\n",
        "        with torch.no_grad():\n",
        "            dummy_out = self.base_model(dummy)\n",
        "        in_features = dummy_out.shape[1]\n",
        "        print(f\"ModelSetup: Determined in_features for final layer: {in_features}\")\n",
        "        self.extension = nn.Linear(in_features, num_classes)\n",
        "        # Có thể chuyển base_model về CPU ở đây nếu muốn, nhưng không bắt buộc\n",
        "        # self.base_model.to('cpu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        base_out = self.base_model(x)\n",
        "        out = self.extension(base_out)\n",
        "        return out\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 3: HÀM HỖ TRỢ (Load model pre-trained) - Sửa lỗi Warning\n",
        "# ==============================================================================\n",
        "def load_pretrained_core_model(name):\n",
        "    \"\"\"Load model pre-trained từ torchvision, ưu tiên dùng weights API.\"\"\"\n",
        "    print(f\"ModelSetup: Loading pre-trained core model: {name}\")\n",
        "    model = None\n",
        "    try:\n",
        "        # Thử tìm Weights enum tương ứng (ví dụ: EfficientNet_B0_Weights)\n",
        "        weights_enum_name = f\"{name.replace('_', '').upper()}_Weights\" # Tạo tên enum chuẩn hơn\n",
        "        weights_enum = getattr(models, weights_enum_name, None)\n",
        "\n",
        "        if weights_enum:\n",
        "            # Lấy weights mặc định (thường là tốt nhất)\n",
        "            weights = weights_enum.DEFAULT\n",
        "            print(f\"Using weights API: {weights}\")\n",
        "            model = getattr(models, name)(weights=weights)\n",
        "        else:\n",
        "            # Nếu không có weights enum (model cũ hoặc tên không khớp), thử pretrained=True (sẽ có warning)\n",
        "            print(f\"Weights enum '{weights_enum_name}' not found. Trying legacy pretrained=True (may show warning).\")\n",
        "            model = getattr(models, name)(pretrained=True)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        # Xử lý nếu getattr(models, name) thất bại\n",
        "        print(f\"[ERROR] Could not load model '{name}' using standard methods: {e}\")\n",
        "        print(\"Trying models.__dict__ fallback...\")\n",
        "        try:\n",
        "            # Fallback rất cũ (ít dùng)\n",
        "             model = models.__dict__[name](pretrained=True)\n",
        "        except Exception as e2:\n",
        "             print(f\"[ERROR] Failed loading model '{name}' completely: {e2}\")\n",
        "             raise # Ném lỗi ra ngoài nếu không thể load\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] An unexpected error occurred loading model '{name}': {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "    if model is None:\n",
        "        raise ValueError(f\"Could not load model for name: {name}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 4: HÀM TẢI DỮ LIỆU (load_preprocessed_data) - Giữ nguyên logic\n",
        "# ==============================================================================\n",
        "def load_preprocessed_data(processed_data_dir, batch_size, num_workers=4, pin_memory=False):\n",
        "    \"\"\"Tải dữ liệu ĐÃ TIỀN XỬ LÝ.\"\"\"\n",
        "    print(f\"\\nLoading preprocessed data from: {processed_data_dir}\")\n",
        "    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    data_transforms_online = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]),\n",
        "        'val_test': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]),\n",
        "    }\n",
        "    image_datasets = {}\n",
        "    for x in ['train', 'val', 'test']:\n",
        "        split_path = os.path.join(processed_data_dir, x)\n",
        "        if not os.path.isdir(split_path): raise FileNotFoundError(f\"Dir not found: {split_path}\")\n",
        "        try:\n",
        "             transform_key = 'train' if x == 'train' else 'val_test'\n",
        "             image_datasets[x] = datasets.ImageFolder(split_path, data_transforms_online[transform_key])\n",
        "             print(f\"Loaded {len(image_datasets[x])} images from {split_path}\")\n",
        "             if len(image_datasets[x]) == 0: print(f\"[Warning] No images loaded for split '{x}'.\")\n",
        "        except Exception as e: print(f\"[ERROR] Load failed for '{x}': {e}\"); raise\n",
        "    dset_classes = image_datasets['train'].classes\n",
        "    print(f\"Classes loaded ({len(dset_classes)}): {dset_classes}\")\n",
        "    dataloaders = {\n",
        "        x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=(x == 'train'),\n",
        "                      num_workers=num_workers, pin_memory=pin_memory)\n",
        "        for x in ['train', 'val', 'test']\n",
        "    }\n",
        "    print(\"Created DataLoaders.\")\n",
        "    return dataloaders['train'], dataloaders['val'], dataloaders['test'], dset_classes\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 5: HÀM ĐÁNH GIÁ MODEL (evaluate_model) - Sửa lỗi Warning AMP\n",
        "# ==============================================================================\n",
        "def evaluate_model(net, dataloader, criterion):\n",
        "    \"\"\"Đánh giá model trên dataloader.\"\"\"\n",
        "    net.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    device = next(net.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False, unit=\"batch\")\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            # !!! Sửa lỗi Warning: Dùng torch.amp.autocast !!!\n",
        "            with torch.amp.autocast(device_type=amp_device_type, enabled=use_gpu):\n",
        "                outputs = net(inputs)\n",
        "                if criterion:\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_batch = (predicted == labels).sum().item()\n",
        "            total_correct += correct_batch\n",
        "            total_samples += labels.size(0)\n",
        "    avg_loss = total_loss / total_samples if criterion and total_samples > 0 else float('nan')\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 6: HÀM HUẤN LUYỆN VÀ VALIDATE (train_and_validate) - Sửa lỗi Warning AMP\n",
        "# ==============================================================================\n",
        "def train_and_validate(net, trainloader, valloader, criterion, epochs, accumulation_steps):\n",
        "    \"\"\"Huấn luyện model, đánh giá trên validation set sau mỗi epoch.\"\"\"\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    # !!! Sửa lỗi Warning: Dùng torch.amp.GradScaler !!!\n",
        "    scaler = torch.amp.GradScaler(device=amp_device_type, enabled=use_gpu)\n",
        "    # --------------------------------------------------\n",
        "    print(f\"\\n--- Starting Training ({epochs} epochs) ---\")\n",
        "    print(f\"Effective Batch Size: {trainloader.batch_size * accumulation_steps}\")\n",
        "    print(f\"Using Mixed Precision (AMP): {use_gpu}\")\n",
        "    device = next(net.parameters()).device\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        total_train = 0\n",
        "        correct_train = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch+1}/{epochs} Train\", unit=\"batch\")\n",
        "        for i, (inputs, labels) in progress_bar:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # !!! Sửa lỗi Warning: Dùng torch.amp.autocast !!!\n",
        "            with torch.amp.autocast(device_type=amp_device_type, enabled=use_gpu):\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                if accumulation_steps > 1: loss = loss / accumulation_steps\n",
        "            # --------------------------------------------------\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(trainloader):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = loss.item() * (accumulation_steps if accumulation_steps > 1 else 1)\n",
        "            running_loss += batch_loss * inputs.size(0)\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                 _, predicted = torch.max(outputs.data, 1)\n",
        "                 correct_train_batch = (predicted == labels).sum().item()\n",
        "                 batch_acc = correct_train_batch / inputs.size(0) if inputs.size(0) > 0 else 0\n",
        "                 correct_train += correct_train_batch\n",
        "                 progress_bar.set_postfix(loss=f\"{batch_loss:.4f}\", acc=f\"{batch_acc:.3f}\")\n",
        "\n",
        "        train_loss = running_loss / total_train if total_train > 0 else float('nan')\n",
        "        train_acc = correct_train / total_train if total_train > 0 else 0.0\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(net, valloader, criterion)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs} Summary -> \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(\"--- Training Finished ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 7: SCRIPT CHÍNH ĐỂ THỰC THI\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\" PLANT DISEASE CLASSIFICATION TRAINING SCRIPT (PREPROCESSED DATA) \".center(60, \"=\"))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.isdir(PROCESSED_DATA_DIR):\n",
        "        print(f\"[FATAL ERROR] Preprocessed data directory not found: {PROCESSED_DATA_DIR}\")\n",
        "        print(\"Please run the preprocessing script ('preprocess_data*.py') first.\")\n",
        "        exit()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name in models_to_test:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Processing Model: {model_name}\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "        acc_steps = accumulation_settings.get(model_name, 1)\n",
        "        model_input_resize = input_sizes[model_name]\n",
        "        print(f\"Config -> Accumulation: {acc_steps}, Model Input Size: {model_input_resize}\")\n",
        "\n",
        "        # --- Tải Dữ liệu ĐÃ TIỀN XỬ LÝ ---\n",
        "        print(\"\\n--- Loading Preprocessed DataLoaders ---\")\n",
        "        try:\n",
        "            train_loader, val_loader, test_loader, dset_classes = load_preprocessed_data(\n",
        "                processed_data_dir=PROCESSED_DATA_DIR,\n",
        "                batch_size=batch_size,\n",
        "                num_workers=num_workers_loader,\n",
        "                pin_memory=use_gpu\n",
        "            )\n",
        "            if num_classes is None: num_classes = len(dset_classes)\n",
        "            elif num_classes != len(dset_classes):\n",
        "                print(f\"[Warning] Class count mismatch. Resetting num_classes to {len(dset_classes)}\")\n",
        "                num_classes = len(dset_classes)\n",
        "            print(f\"Number of classes for model: {num_classes}\")\n",
        "\n",
        "        except Exception as e: print(f\"[ERROR] Load data failed: {e}\"); traceback.print_exc(); continue\n",
        "\n",
        "        # --- Cài đặt Model ---\n",
        "        try:\n",
        "            print(f\"\\nSetting up {model_name}...\")\n",
        "            base_model = load_pretrained_core_model(model_name)\n",
        "            model = ExtendedModel(base_model, num_classes, input_size=model_input_resize)\n",
        "\n",
        "            # --- Gợi ý tối ưu tốc độ: Thử torch.compile ---\n",
        "            try:\n",
        "                # Chỉ dùng nếu PyTorch version >= 2.0\n",
        "                # print(\"Attempting to compile model with torch.compile()...\")\n",
        "                # model = torch.compile(model)\n",
        "                # print(\"Model compiled successfully.\")\n",
        "                pass # Bỏ comment các dòng trên để kích hoạt\n",
        "            except Exception as compile_err:\n",
        "                print(f\"[Warning] torch.compile() failed: {compile_err}. Proceeding without compiling.\")\n",
        "            # --------------------------------------------\n",
        "\n",
        "            if use_gpu:\n",
        "                if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
        "                model = model.cuda()\n",
        "\n",
        "        except Exception as e: print(f\"[ERROR] Model setup failed: {e}\"); traceback.print_exc(); continue\n",
        "\n",
        "        # --- Huấn luyện và Đánh giá ---\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        if use_gpu: criterion = criterion.cuda()\n",
        "\n",
        "        try:\n",
        "            train_and_validate(\n",
        "                net=model, trainloader=train_loader, valloader=val_loader,\n",
        "                criterion=criterion, epochs=epochs_to_train, accumulation_steps=acc_steps\n",
        "            )\n",
        "        except Exception as e: print(f\"[ERROR] Train/Val failed: {e}\"); traceback.print_exc(); print(\"Skipping testing.\"); continue\n",
        "\n",
        "        # --- Test cuối cùng ---\n",
        "        print(\"\\n--- Final Testing Phase ---\")\n",
        "        try:\n",
        "            test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
        "            print(f\"\\n>>> Test Results for {model_name}: Loss = {test_loss:.4f}, Accuracy = {test_acc:.4f}\")\n",
        "            results[model_name] = {'test_loss': test_loss, 'test_accuracy': test_acc}\n",
        "        except Exception as e: print(f\"[ERROR] Test failed: {e}\"); traceback.print_exc(); results[model_name] = {'test_loss': float('nan'), 'test_accuracy': float('nan')}\n",
        "\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        # --- Dọn dẹp bộ nhớ ---\n",
        "        print(f\"Cleaning up resources for {model_name}...\")\n",
        "        del model, base_model, train_loader, val_loader, test_loader, criterion\n",
        "        if use_gpu: torch.cuda.empty_cache()\n",
        "\n",
        "    # --- In tổng kết ---\n",
        "    print(\"\\n========================================\")\n",
        "    print(\" TRAINING & TESTING COMPLETE \".center(40, \"=\"))\n",
        "    print(\"========================================\")\n",
        "    print(\"Final Test Results Summary:\")\n",
        "    if results:\n",
        "        for model_name, metrics in results.items():\n",
        "             print(f\"- {model_name}: Test Loss={metrics['test_loss']:.4f}, Test Accuracy={metrics['test_accuracy']:.4f}\")\n",
        "    else:\n",
        "        print(\"No models were successfully processed.\")\n",
        "    print(\"=\"*40)\n",
        "    print(\"SCRIPT FINISHED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "39ETN5F82oED",
        "outputId": "e9da82ea-286b-4226-a498-90f09853a243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA not available. Using CPU.\n",
            "============================================================\n",
            " PLANT DISEASE CLASSIFICATION TRAINING SCRIPT (PREPROCESSED DATA) \n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "Processing Model: vit_b_16\n",
            "==================================================\n",
            "\n",
            "Config -> Accumulation: 4, Model Input Size: (224, 224)\n",
            "\n",
            "--- Loading Preprocessed DataLoaders ---\n",
            "\n",
            "Loading preprocessed data from: /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed\n",
            "Loaded 44018 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/train\n",
            "Loaded 5502 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/val\n",
            "Loaded 5502 images from /content/drive/MyDrive/Colab Notebooks/PlantVillage_Processed/test\n",
            "Classes loaded (39): ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'background']\n",
            "Created DataLoaders.\n",
            "Number of classes for model: 39\n",
            "\n",
            "Setting up vit_b_16...\n",
            "ModelSetup: Loading pre-trained core model: vit_b_16\n",
            "Weights enum 'VITB16_Weights' not found. Trying legacy pretrained=True (may show warning).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:04<00:00, 80.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelSetup: Determined in_features for final layer: 1000\n",
            "\n",
            "--- Starting Training (10 epochs) ---\n",
            "Effective Batch Size: 128\n",
            "Using Mixed Precision (AMP): False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 Train:   0%|          | 4/1376 [06:09<35:10:29, 92.30s/batch, acc=0.062, loss=3.9003]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4ec01db1b83e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             train_and_validate(\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_to_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4ec01db1b83e>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(net, trainloader, valloader, criterion, epochs, accumulation_steps)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;31m# --------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}